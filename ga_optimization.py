# -*- coding: utf-8 -*-
"""genetic-algorithm-python-tutorial.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/161ijkvn8wG_seVtQexm-p3fW3r5p8s_x
# Genetic Algorithm Implementation with Python
* Tutorial: https://towardsai.net/p/computer-science/genetic-algorithm-ga-introduction-with-example-code-e59f9bc58eaf
* Github: https://github.com/towardsai/tutorials/tree/master/genetic-algorithm-tutorial
The Genetic Algorithm is a class of evolutionary algorithm that is broadly inspired by biological evolution. We all know evolution, it is a selection of parents, reproduction, and mutation of offsprings. The main aim of evolution is to reproduce offsprings that are biologically better than their parents. Genetic algorithm is mainly based on natural selection and it tries to simulate the theory of evolution.
"""

import numpy as np
import matplotlib.pyplot as plt
import copy
from numpy.random import randint
from numpy.random import rand
import torch
import argparse
from fast_jtnn import *
from robot_utils import *
import numpy as np
import robot_utils.tasks as tasks
import pyrobotdesign as rd
from design_search import make_graph, build_normalized_robot
import random
import matplotlib.pyplot as plt
# from uniform_reward_net_total import RewardNet_terrain
import threading
import os 

parser = argparse.ArgumentParser()
parser.add_argument('--nsample', type=int, default=1)
# parser.add_argument('--model', required=True)
# parser.add_argument('--model', type=str, default="sum_ls28_pred20/model.iter-900000")
parser.add_argument('--model', type=str, default="sum_ls28_pred20/model.iter-400000")
parser.add_argument('--hidden_size', type=int, default=450)
parser.add_argument('--latent_size', type=int, default=28)
parser.add_argument('--depthT', type=int, default=20)

parser.add_argument("--grammar_file", type=str, default="data/designs/grammar_apr30.dot",
                    help="Grammar file (.dot)")
parser.add_argument("-j", "--jobs", type=int, default=16,
                        help="Number of jobs/threads")

parser.add_argument("--task", type=str, default="FlatTerrainTask", help="Task (Python class name)")
parser.add_argument("-l", "--episode_len", type=int, default=512,
                        help="Length of episode")

parser.add_argument('--encode', type=str, default="sum")
parser.add_argument("--pred", default=True, action="store_false")
args = parser.parse_args()

def sample_graph(model):
    root, pred_nodes = model.sample_prior()
    n_nodes = len(pred_nodes)
    adj_matrix_np = np.zeros([n_nodes, n_nodes])
    features_np = np.zeros(n_nodes)
    idx_offset = root.idx
    for i in range(n_nodes):
        node = pred_nodes[i]
        features_np[i] = node.wid
        for nei in node.neighbors:
            true_idx = nei.idx - idx_offset
            adj_matrix_np[true_idx, i] = 1
            adj_matrix_np[i, true_idx] = 1
    return adj_matrix_np, features_np,

def decode_graph(model, tree_vec):
    root, pred_nodes = model.decode(tree_vec, prob_decode=False)
    n_nodes = len(pred_nodes)
    adj_matrix_np = np.zeros([n_nodes, n_nodes])
    features_np = np.zeros(n_nodes)
    idx_offset = root.idx
    for i in range(n_nodes):
        node = pred_nodes[i]
        features_np[i] = node.wid
        for nei in node.neighbors:
            true_idx = nei.idx - idx_offset
            adj_matrix_np[true_idx, i] = 1
            adj_matrix_np[i, true_idx] = 1
    return adj_matrix_np, features_np


def get_robot_image(robot, task):
    sim = rd.BulletSimulation(task.time_step)
    task.add_terrain(sim)
    viewer = rd.GLFWViewer()
    if robot is not None:
        robot_init_pos, _ = presimulate(robot)
        # Rotate 180 degrees around the y axis, so the base points to the right
        sim.add_robot(robot, robot_init_pos, rd.Quaterniond(0.0, 0.0, 1.0, 0.0))
        robot_idx = sim.find_robot_index(robot)

        # Get robot position and bounds
        base_tf = np.zeros((4, 4), order='f')
        lower = np.zeros(3)
        upper = np.zeros(3)
        sim.get_link_transform(robot_idx, 0, base_tf)
        sim.get_robot_world_aabb(robot_idx, lower, upper)
        viewer.camera_params.position = base_tf[:3,3]
        viewer.camera_params.yaw = - np.pi / 3
        viewer.camera_params.pitch = -np.pi / 4.5
        viewer.camera_params.distance = np.linalg.norm(upper - lower) * 1.5
    else:
        viewer.camera_params.position = [1.0, 0.0, 0.0]
        viewer.camera_params.yaw = -np.pi / 3
        viewer.camera_params.pitch = -np.pi / 6
        viewer.camera_params.distance = 5.0

    viewer.update(task.time_step)
    return viewer.render_array(sim)
opt_seed = random.getrandbits(32)
rule_list = {"RidgedTerrainTask":
                "0, 7, 1, 13, 1, 2, 16, 12, 13, 6, 4, 19, 4, 17, 5, 3, 2, 16, 4, 5, 18, 9, 8, 9, 9, 8",
            "FlatTerrainTask":
                "0, 12, 7, 1, 12, 3, 10, 1, 3, 1, 12, 12, 1, 3, 10, 2, 16, 8, 1, 3, "
                "12, 4, 1, 3, 2, 12, 18, 9, 18, 8, 5, 5, 1, 12, 6, 3",
            "GapTerrainTask":
                "0, 1, 1, 7, 1, 6, 10, 3, 2, 4, 10, 10, 3, 16, 4, 16, "
                "18, 2, 5, 16, 8, 4, 8, 8, 18, 4, 5, 15, 9, 8, 8",
            "FrozenLakeTask":
                "0, 1, 1, 1, 6, 7, 10, 11, 13, 2, 4, 3, 4, 16, 8, 14, 4, 8, 3, 15, 15, 5, 3, 9, 8"}
print(rule_list)

def get_distance(robot, task):
        robot_init_pos, has_self_collision = presimulate(robot)

        # finding distance start from here
        if has_self_collision:
            return 0

        def make_sim_fn():
            sim = rd.BulletSimulation(task.time_step)
            task.add_terrain(sim)
            # Rotate 180 degrees around the y axis, so the base points to the right
            sim.add_robot(robot, robot_init_pos, rd.Quaterniond(0.0, 0.0, 1.0, 0.0))
            return sim

        main_sim = make_sim_fn()
        robot_idx = main_sim.find_robot_index(robot)

        dof_count = main_sim.get_robot_dof_count(robot_idx)
        
        episode_count = 1
        if episode_count >= 2:
            value_estimator = rd.FCValueEstimator(main_sim, robot_idx, 'cpu', 64, 3, 1)
        else:
            value_estimator = rd.NullValueEstimator()
        input_sampler = rd.DefaultInputSampler()
        objective_fn = task.get_objective_fn()

        replay_obs = np.zeros((value_estimator.get_observation_size(), 0))
        replay_returns = np.zeros(0)

        for episode_idx in range(episode_count):
            optimizer = rd.MPPIOptimizer(1.0, task.discount_factor, dof_count,
                                        task.interval, task.horizon, 512,
                                        12, opt_seed + episode_idx,
                                        make_sim_fn, objective_fn, value_estimator,
                                        input_sampler)

            optimizer.update()
            optimizer.set_sample_count(64)

            main_sim.save_state()
            input_sequence = np.zeros((dof_count, task.episode_len))
            obs = np.zeros((value_estimator.get_observation_size(),
                            task.episode_len + 1), order='f')
            rewards = np.zeros(task.episode_len * task.interval)

            lower = np.zeros(3)
            upper = np.zeros(3)
            for j in range(task.episode_len):
                optimizer.update()
                input_sequence[:, j] = optimizer.input_sequence[:, 0]
                optimizer.advance(1)
                # print(type(main_sim), type(obs[:,j]), obs[:, j].shape)
                # value_estimator.get_observation(main_sim, obs[:, j])
                value_estimator.get_observation(main_sim, np.expand_dims(obs[:, j], axis=1))
                if j == 0:
                    main_sim.get_robot_world_aabb(robot_idx, lower, upper)
                    init_lower = lower.copy()
                    init_upper = upper.copy()
                    # print(init_lower, init_upper)
                if (j == task.episode_len -1):
                    main_sim.get_robot_world_aabb(robot_idx, lower, upper)
                    # print(lower, upper)
                    # print("diff", lower-init_lower, upper-init_upper)
                    # print("Center diff", (lower+upper)/2 - (init_lower+init_upper)/2)
                    dis = ((lower+upper)/2 - (init_lower+init_upper)/2)[0]
                for k in range(task.interval):
                    main_sim.set_joint_targets(robot_idx,
                                            input_sequence[:, j].reshape(-1, 1))
                    task.add_noise(main_sim, j * task.interval + k)
                    main_sim.step()
                    rewards[j * task.interval + k] = objective_fn(main_sim)
            # value_estimator.get_observation(main_sim, obs[:, -1])
            value_estimator.get_observation(main_sim, np.expand_dims(obs[:, -1], axis=1))

            main_sim.restore_state()
        print("The dis is",dis)

        camera_params, record_step_indices = view_trajectory(
            main_sim, robot_idx, input_sequence, task)
        return dis
# objective function
def private_simulation_objective(x,k,reward_model):
	# return x[0]**2.0 + x[1]**2.0 + x[2]**2.0 + x[3]**2.0
    # print(-x[0]**2)
    # return -x[0]**2
    X = torch.Tensor(x).reshape(1,-1).to(device)
    
    x_env = np.zeros((X.shape[0], 9))
    x_env[:,k] += 1
    X_env = torch.Tensor(x_env).to(device)

    adj_matrix_np, features_np  = decode_graph(model, X.float().cuda())
    num_modules = features_np.shape[0]
    
    task_class = getattr(tasks, "FlatTerrainTask")
    robot1 = graph_to_robot(adj_matrix_np, features_np)
    task = task_class(episode_len=args.episode_len)
    # seq, score = simulate(robot1, task, opt_seed, 12, 1)
    score = get_distance(robot1, task)
    # return -reward_model(X, X_env).item(), reward_model(X, X_env).item(), (20 - num_modules)/6 
    return -score, reward_model(X, X_env).item(), (20 - num_modules)/6 

def private_ga_objective(x,k,reward_model):
    X = torch.Tensor(x).reshape(1,-1).to(device)
    
    x_env = np.zeros((X.shape[0], 9))
    x_env[:,k] += 1
    # x_env = np.zeros((X.shape[0], 1, 720, 280))
    # x_env[:] = terrain_array_dict[k]
    X_env = torch.Tensor(x_env).to(device)

    adj_matrix_np, features_np  = decode_graph(model, X.float().cuda())
    num_modules = features_np.shape[0]

    return -reward_model(X, X_env).item(), reward_model(X, X_env).item(), (20 - num_modules)/6 

def objective(x,k,reward_model):
	# return x[0]**2.0 + x[1]**2.0 + x[2]**2.0 + x[3]**2.0
    # print(-x[0]**2)
    # return -x[0]**2
    X = torch.Tensor(x).reshape(1,-1)
    
    x_env = np.zeros((X.shape[0], 9))
    x_env[:,k] += 1
    X_env = torch.Tensor(x_env)

    adj_matrix_np, features_np  = decode_graph(model, X.float().cuda())
    num_modules = features_np.shape[0]

    return -reward_model(X, X_env).item(), reward_model(X, X_env).item(), (20 - num_modules)/6 

# cost function
def sphere(x):
  ''' This is the problem we will be
  optimizing, each chromosome of parent has a cost
  which is calculated from this cost function'''

  return sum(x**2)

def roulette_wheel_selection(p):
  ''' Roulette Wheel Selection is a method of parent
  selection for breeding. We take the cummulative sum of probabilities
  and select the first parent whose cummulative sum is greater than
  random number'''

  c = np.cumsum(p)
  r = sum(p) * np.random.rand()
  ind = np.argwhere(r <= c)

  return ind[0][0]

def crossover(p1, p2):
  ''' Performing uniform crossover. Alpha is the flag
  that determines which gene of each chromosome is choosen
  to be inherited by the offspring. Maultiply the alpha value
  with each gene of every chromosome of both the parents and
  then add the resultant value to get child chromosome'''

  c1 = copy.deepcopy(p1)
  c2 = copy.deepcopy(p2)

  # Uniform crossover
  alpha = np.random.uniform(0, 1, *(c1['position'].shape))
  c1['position'] = alpha*p1['position'] + (1-alpha)*p2['position']
  c2['position'] = alpha*p2['position'] + (1-alpha)*p1['position']

  return c1, c2

def mutate(c, mu, sigma):
  '''
  c: child chromosome
  mu: mutation rate. % of gene to be modified
  sigma: step size of mutation'''

  y = copy.deepcopy(c)
  flag = np.random.rand(*(c['position'].shape)) <= mu  # array of True and Flase, indicating at which position to perform mutation
  ind = np.argwhere(flag)
  y['position'][ind] += sigma * np.random.randn(*ind.shape)

  return y

def bounds(c, varmin, varmax):
  ''' Defines the upper and lower bound of gene value'''

  c['position'] = np.maximum(c['position'], varmin)
  c['position'] = np.minimum(c['position'], varmax)

def sort(arr):
  ''' Bubble sorting the population + offsoring
  in every iteration to get best fit individuals at top'''

  n = len(arr)

  for i in range(n-1):

    for j in range(0, n-i-1):
            if arr[j]['cost'] > arr[j+1]['cost'] :
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr

def ga(costfunc, num_var, varmin, varmax, maxit, npop, num_children, mu, sigma, beta):

  # Placeholder for each individual
  population = {}
  for i in range(npop):                                                         # each inidivdual has position(chromosomes) and cost,
    population[i] = {'position': None, 'cost': None}                            # create individual as many as population size(npop)

  # Best solution found
  bestsol = copy.deepcopy(population)
  bestsol_cost = np.inf                                                         # initial best cost is infinity

  # Initialize population - 1st Gen
  for i in range(npop):
      population[i]['position'] = np.random.uniform(varmin, varmax, num_var)    # randomly initialize the chromosomes and cost
      population[i]['cost'] = costfunc(population[i]['position'])

      if population[i]['cost'] < bestsol_cost:                                  # if cost of an individual is less(best) than best cost,
        bestsol = copy.deepcopy(population[i])                                  # replace the best solution with that individual

  # Best cost of each generation/iteration
  bestcost = np.empty(maxit)

  # Main loop
  for it in range(maxit):

    # Calculating probability for roulette wheel selection
    costs = []
    for i in range(len(population)):
      costs.append(population[i]['cost'])                                       # list of all the population cost
    costs = np.array(costs)
    avg_cost = np.mean(costs)                                                   # taking average of the costs
    if avg_cost != 0:
      costs = costs/avg_cost
    probs = np.exp(-beta*costs)                                                 # probability is exponensial of -ve beta times costs

    for _ in range(num_children//2):                                            # we will be having two off springs for each crossover
                                                                                # hence divide number of children by 2
      '''
      -> choosing two parents randomly for mating
      -> we are shuffling all the 20 parent individuals and
      -> choosing first two of the shuffled array as our parents for mating
      Randomly selecting parents by shiffling them.
      But we will be using roulette wheel slection
      for our algorithm
      q = np.random.permutation(npop)
      p1 = population[q[0]]
      p2 = population[q[1]]
      '''

      # Roulette wheel selection
      p1 = population[roulette_wheel_selection(probs)]
      p2 = population[roulette_wheel_selection(probs)]

      # crossover two parents
      c1, c2 = crossover(p1, p2)

      # Perform mutation
      c1 = mutate(c1, mu, sigma)
      c2 = mutate(c2, mu, sigma)
      # Apply bounds
      bounds(c1, varmin, varmax)
      bounds(c2, varmin, varmax)
      # Evaluate first off spring
      c1['cost'] = costfunc(c1['position'])                                     # calculate cost function of child 1

      if type(bestsol_cost) == float:
        if c1['cost'] < bestsol_cost:                                           # replacing best solution in every generation/iteration
          bestsol_cost = copy.deepcopy(c1)
      else:
        if c1['cost'] < bestsol_cost['cost']:                                   # replacing best solution in every generation/iteration
          bestsol_cost = copy.deepcopy(c1)


      # Evaluate second off spring
      if c2['cost'] < bestsol_cost['cost']:                                     # replacing best solution in every generation/iteration
        bestsol_cost = copy.deepcopy(c2)

    # Merge, Sort and Select
    population[len(population)] = c1
    population[len(population)] = c2

    population = sort(population)

    # Store best cost
    bestcost[it] = bestsol_cost['cost']

    # Show generation information
    print('Iteration {}: Best Cost = {}'. format(it, bestcost[it]))


  out = population
  Bestsol = bestsol
  bestcost = bestcost
  return (out, Bestsol, bestcost)

def ga_one_iter(costfunc, num_var, varmin, varmax, maxit, npop, num_children, mu, sigma, beta, init_pop, rewardNet, k):

    # Placeholder for each individual
    population = {}
    for i in range(npop):                                                         # each inidivdual has position(chromosomes) and cost,
        population[i] = {'position': None, 'cost': None, \
                        'dist_score': None, 'mod_score': None}                            # create individual as many as population size(npop)

    # Best solution found
    bestsol = copy.deepcopy(population)
    bestsol_cost = np.inf                                                         # initial best cost is infinity

    # Initialize population - 1st Gen
    for i in range(npop):
        # population[i]['position'] = np.random.uniform(varmin, varmax, num_var)    # randomly initialize the chromosomes and cost
        population[i]['position'] = init_pop
        # population[i]['cost'] = costfunc(population[i]['position'])
        population[i]['cost'],population[i]['dist_score'],population[i]['mod_score'] = \
        costfunc(population[i]['position'],k,rewardNet)

        if population[i]['cost'] < bestsol_cost:                                  # if cost of an individual is less(best) than best cost,
            bestsol = copy.deepcopy(population[i])                                  # replace the best solution with that individual

    # Best cost of each generation/iteration
    bestcost = np.empty(maxit)

    # Main loop
    for it in range(maxit):

        # Calculating probability for roulette wheel selection
        costs = []
        for i in range(len(population)):
            costs.append(population[i]['cost'])                                       # list of all the population cost
        costs = np.array(costs)
        avg_cost = np.mean(costs)                                                   # taking average of the costs
        if avg_cost != 0:
            costs = costs/avg_cost
        probs = np.exp(-beta*costs)                                                 # probability is exponensial of -ve beta times costs

        for _ in range(num_children//2):                                            # we will be having two off springs for each crossover
                                                                                    # hence divide number of children by 2
            '''
            -> choosing two parents randomly for mating
            -> we are shuffling all the 20 parent individuals and
            -> choosing first two of the shuffled array as our parents for mating
            Randomly selecting parents by shiffling them.
            But we will be using roulette wheel slection
            for our algorithm
            q = np.random.permutation(npop)
            p1 = population[q[0]]
            p2 = population[q[1]]
            '''

            # Roulette wheel selection
            p1 = population[roulette_wheel_selection(probs)]
            p2 = population[roulette_wheel_selection(probs)]

            # crossover two parents
            c1, c2 = crossover(p1, p2)

            # Perform mutation
            c1 = mutate(c1, mu, sigma)
            c2 = mutate(c2, mu, sigma)
            # Apply bounds
            bounds(c1, varmin, varmax)
            bounds(c2, varmin, varmax)
            # Evaluate first off spring
            # c1['cost'] = costfunc(c1['position']) 
            c1['cost'],c1['dist_score'],c1['mod_score'] = costfunc(c1['position'],k,rewardNet)                                     # calculate cost function of child 1

            if type(bestsol_cost) == float:
                if c1['cost'] < bestsol_cost:                                           # replacing best solution in every generation/iteration
                    bestsol_cost = copy.deepcopy(c1)
            else:
                if c1['cost'] < bestsol_cost['cost']:                                   # replacing best solution in every generation/iteration
                    bestsol_cost = copy.deepcopy(c1)


            # Evaluate second off spring
            if c2['cost'] < bestsol_cost['cost']:                                     # replacing best solution in every generation/iteration
                bestsol_cost = copy.deepcopy(c2)

        # Merge, Sort and Select
        population[len(population)] = c1
        population[len(population)] = c2

        population = sort(population)

        # Store best cost
        bestcost[it] = bestsol_cost['cost']
        bestsol = bestsol_cost['position']
        # Show generation information
        # fig_root_path = "ga_figs_dis"
        
        # if not os.path.exists(fig_root_path):
        #     os.mkdir(fig_root_path)
        # save_fig_path = os.path.join(fig_root_path,"GA_design_k_{}_iter{}_{}.png".format(k, it, "{:.0f}".format(bestcost[it]*100)))
        # adj_matrix_np, features_np  = decode_graph(model, torch.tensor(bestsol, dtype=torch.float).cuda().reshape(1,-1))
        # robot1 = graph_to_robot(adj_matrix_np, features_np)
        # plt.imsave(save_fig_path, get_robot_image(robot1, task))
        # print(bestsol)
        # print('Iteration {}: Best Cost = {}'. format(it, bestcost[it])) # Uncomment for the debugging

    out = population
    Bestsol = bestsol
    bestcost = bestcost
    # return (out, Bestsol, bestcost)
    print("Best from rw", -bestcost[-1])
    return bestsol, -bestcost[-1], bestsol_cost['dist_score'], bestsol_cost['mod_score']

def multithreading_simulate(robot1, task, opt_seed, jobs, val, res, idx):
    seq, score = simulate(robot1, task, opt_seed, jobs, 1)
    res[idx] = (seq, score)

if __name__ == "__main__":
    
    from uniform_reward_net_total import RewardNet
    from uniform_reward_net_total_onehot import RewardNet_onehot
    import torch.optim as optim
    import time
    cuda = True if torch.cuda.is_available() else False
    device = torch.device("cuda" if cuda else "cpu")
    model = JTNNVAE(args.hidden_size, args.latent_size, args.depthT, args.encode, args.pred)
    model.load_state_dict(torch.load(args.model))
    model = model.cuda()
    task_class = getattr(tasks, "FlatTerrainTask")
    FlatTerrainTask = task_class(episode_len=256)
    task_class = getattr(tasks, "RidgedTerrainTask")
    RidgedTerrainTask = task_class(episode_len=256)
    task_class = getattr(tasks, "GapTerrainTask")
    GapTerrainTask = task_class(episode_len=256)

    task_class = getattr(tasks, "CustomizedWallTerrainTask1")
    CustomizedWallTerrainTask1 = task_class(episode_len=256)
    task_class = getattr(tasks, "CustomizedWallTerrainTask2")
    CustomizedWallTerrainTask2 = task_class(episode_len=256)
    task_class = getattr(tasks, "CustomizedSteppedTerrainTask2")
    CustomizedSteppedTerrainTask2 = task_class(episode_len=300)

    task_class = getattr(tasks, "CustomizedBiModalTerrainTask1")
    CustomizedBiModalTerrainTask1 = task_class(episode_len=256)
    task_class = getattr(tasks, "CustomizedBiModalTerrainTask2")
    CustomizedBiModalTerrainTask2 = task_class(episode_len=200)
    task_class = getattr(tasks, "HillTerrainTask")
    HillTerrainTask = task_class(episode_len=200)

    task_class = getattr(tasks, "CustomizedSteppedTerrainTask1")
    CustomizedSteppedTerrainTask1 = task_class(episode_len=512)

    task_class = getattr(tasks, "CustomizedBiModalTerrainTask3")
    CustomizedBiModalTerrainTask3 = task_class(episode_len=300)


    # # ckpt = torch.load('ckpt/reward_net_3k.pt')
    # ckpt = torch.load('ckpt_exp/uniform_reward_net_500k_1k_4envs.pt')
    # reward_model = RewardNet(input_length = 28, n_hidden_layers=2, hidden_layer_size = 128, env_size=4)
    # reward_model = RewardNet_onehot(input_length = 28, n_hidden_layers=2, hidden_layer_size = 128, env_size = 9).to(device)
    # reward_learning_rate = 0.002
    # reward_weight_decay = 5e-6
    # reward_optimizer = optim.Adam(reward_model.parameters(), lr=reward_learning_rate, weight_decay=reward_weight_decay)

    # ckpt = torch.load('ckpt_exp/uniform_reward_net_500k_1k_4envs_onehot_best.pt')
    # # reward_model.load_state_dict(torch.load('ckpt/reward_net_1k_4envs.pt'))
    # reward_model.load_state_dict(ckpt['model_state_dict'])
    # reward_model.eval()
    opt_seed = 42
    # Problem definition
    # costfunc = private_ga_objective
    costfunc = private_simulation_objective
    num_var = 28       # number of decicion variables
    # varmin = -5      # lower bound
    # varmax = 5       # upper bound
    bds = np.load("bounds_500k.npy")
    # print(bds[:,0])
    # print(bds[:,1])
    varmin = bds[:,0]
    varmax = bds[:,1]

    # GA Parameters
    maxit = 50                                              # number of iterations
    npop = 5                                                # initial population size
    beta = 1
    prop_children = 1                                        # proportion of children to population
    num_children = int(np.round(prop_children * npop/2)*2)   # making sure it always an even number
    mu = 0.2                                                 # mutation rate 20%, 205 of 5 is 1, mutating 1 gene
    sigma = 0.1                                              # step size of mutation


    test_iter = 1
    corrects = []
    for k in range(9):
        k = 3
        cnt = 0
        if k == 0:
            task_name = 'FlatTerrainTask'
            task = FlatTerrainTask
        if k == 1:
            task_name = 'RidgedTerrainTask'
            task = RidgedTerrainTask
        if k == 2:
            task_name = 'GapTerrainTask'
            task = GapTerrainTask
        if k == 3:
            task_name = 'CustomizedWallTerrainTask1'
            task = CustomizedWallTerrainTask1
        if k == 4:
            task_name = 'CustomizedWallTerrainTask2'
            task = CustomizedWallTerrainTask2
        if k == 5:
            task_name = 'CustomizedSteppedTerrainTask2'
            task = CustomizedSteppedTerrainTask2
        if k == 6:
            task_name = 'CustomizedBiModalTerrainTask1'
            task = CustomizedBiModalTerrainTask1
        if k == 7:
            task_name = 'CustomizedBiModalTerrainTask2'
            task = CustomizedBiModalTerrainTask2
        if k == 8:
            task_name = 'HillTerrainTask'
            task = HillTerrainTask
        if k == 9:
            task_name = 'CustomizedSteppedTerrainTask1'
            task = CustomizedSteppedTerrainTask1
        if k == 10:
            task_name = 'CustomizedBiModalTerrainTask3'
            task = CustomizedBiModalTerrainTask3
        designs_path = "GAN_ckpt_v24_epoch10_3envs_uniform_cst1_512/generated_designs_epoch10/GAN_result_design_500_v4_4env_SteppedTerrainTask.npy"
        dis_path = "GAN_ckpt_v24_epoch10_3envs_uniform_cst1_512/generated_designs_epoch10/GAN_result_dis_500_v4_4env_SteppedTerrainTask.npy"
        mod_path = "GAN_ckpt_v24_epoch10_3envs_uniform_cst1_512/generated_designs_epoch10/GAN_result_mod_500_v4_4env_SteppedTerrainTask.npy"
        designs = np.load(designs_path)
        dis_list = np.load(dis_path)
        idx = np.argmax(dis_list)
        vec = designs[idx]
        # vec = [1.52133879, -0.60356692,  0.79820866,  0.62134996, -1.79127588,  0.84719664,
        #     -0.88428103, -0.22939346,  2.13711127,  1.92968188, -0.53965231,  0.36267054,
        #     -0.36921255,  1.47811633,  0.76874351, -0.54804625, -1.65168565,  0.75588401,
        #     -1.81797097, -1.06523969,  1.02309758,  0.03863148,  1.03553172, -1.28412874,
        #     0.01284375,  0.13603959,  1.54526541, -0.93015268]
        vec = np.array(vec)
        for i in range(test_iter):
            # task_class = getattr(tasks, args.task)
            # task = task_class(episode_len=args.episode_len)

            # Run GA
            # out = ga(costfunc, num_var, varmin, varmax, maxit, npop, num_children, mu, sigma, beta)
            # init = np.random.uniform(varmin, varmax, num_var)
            # init = [-0.42105492,  2.07314603, -0.81671534,  2.05360516,  0.25225775,  1.89400168,\
            #         -1.56602309, -0.28385988, -0.81614669, -1.19536242, -2.68024078,  0.54721525,\
            #         -2.95561138,  2.1500398,  -1.55778335,  0.83260687,  1.46759942,  1.93973844,\
            #         -1.17244679, -2.02176059, -2.45239294, -2.30022974,  2.50066512,  1.8945962,\
            #         2.887226,    1.62168349,  0.60159975, -1.47454025,]
            # init = np.array(init)
            init = vec.reshape(-1).tolist()
            print(init)
            vec = torch.tensor(init).float().reshape(1, -1).cuda()
            adj_matrix_np, features_np  = decode_graph(model, vec)
            robot1 = graph_to_robot(adj_matrix_np, features_np)
            fig_root_path = "ga_figs_dis"

            input_sequence1, result1 = simulate(robot1, task, opt_seed, args.jobs, 1)
            print("init", result1, private_ga_objective(init,0,reward_model),private_simulation_objective(init,k,reward_model))
            if not os.path.exists(fig_root_path):
                os.mkdir(fig_root_path)
            save_fig_path = os.path.join(fig_root_path,"GA_design_k_init_{}.png".format(k, "{:.0f}".format(result1*100)))

            plt.imsave(save_fig_path, get_robot_image(robot1, task))
            start = time.time()
            # print(start)

            # designs_path = "GAN_ckpt_v25_epoch10_3envs_uniform_cst1_512/generated_designs_epoch8/GAN_result_design_500_v4_4env_SteppedTerrainTask.npy"
            # designs = np.load(designs_path)


            # dis_path = "GAN_ckpt_v25_epoch10_3envs_uniform_cst1_512/generated_designs_epoch8/GAN_result_dis_500_v4_4env_SteppedTerrainTask.npy"
            # dis_list = np.load(dis_path)
            # dis_list_uni = np.unique(dis_list)
            # print(dis_list_uni)

            # mod_path = "GAN_ckpt_v25_epoch10_3envs_uniform_cst1_512/generated_designs_epoch8/GAN_result_mod_500_v4_4env_SteppedTerrainTask.npy"

            # mod_list = np.load(mod_path)
            # mod_list_unique = np.unique(mod_list)

            # idx = np.argmax(dis_list)
            # print(idx, mod_list[idx])
            # init = designs[idx]

            res, score, dist_score, mod_score = ga_one_iter(costfunc, num_var, varmin, varmax, maxit, npop, num_children, mu, sigma, beta,init, reward_model, k=k)
            # print("~~~~~~~")
            print("ga out", res, score, dist_score, mod_score)
            # print(score, dist_score)
            end = time.time()
            vec = torch.tensor(res).float().reshape(1, -1).cuda()
            adj_matrix_np, features_np  = decode_graph(model, vec)
            robot1 = graph_to_robot(adj_matrix_np, features_np)
            

            # vec = torch.tensor(res).float().reshape(1, -1).cuda()
            # adj_matrix_np, features_np  = decode_graph(model, vec)
            # robot2 = graph_to_robot(adj_matrix_np, features_np)

            input_sequence1, result2 = simulate(robot1, task, opt_seed, args.jobs, 1)

            print("after",result2, private_ga_objective(res,0,reward_model))
            
            save_fig_path = os.path.join(fig_root_path,"GA_design_k_final_{}.png".format(k, "{:.0f}".format(result2*100)))
            plt.imsave(save_fig_path, get_robot_image(robot1, task))
            print("time:",end - start)
            if result1 < result2:
                cnt += 1
        print("{} out of {} in task {}".format(cnt, test_iter, k))
        corrects.append(cnt)
        break
    print(corrects)
    # # input_sequence2, result2 = simulate(robot2, task, opt_seed, args.jobs, 1)
    # res_l = [([0]*28,0) for _ in range(2)]
    # t1 = threading.Thread(target=multithreading_simulate, args=(robot1, task, opt_seed, args.jobs, 1, res_l, 0), name='t1')
    # # t3 = threading.Thread(target=multithreading_simulate, args=(robot1, task, opt_seed, args.jobs, 1, res_l, 2), name='t3')
    # # t4 = threading.Thread(target=multithreading_simulate, args=(robot1, task, opt_seed, args.jobs, 1, res_l, 3), name='t4')
    # t2 = threading.Thread(target=multithreading_simulate, args=(robot2, task, opt_seed, args.jobs, 1, res_l, 1), name='t2')
    # t1.start()
    # t2.start()
    # # t3.start()
    # # t4.start()
    # t1.join()
    # t2.join()
    # # t3.join()
    # # t4.join()
    # input_sequence1, result1 = res_l[0]
    # input_sequence2, result2 = res_l[1]
    # print(result1)
    # print(result2, score)
    # print(init)
    # print(res)
    # # plt.imshow(get_robot_image(robot, task), origin='lower')
    # # plt.show()
    # robot_init_pos, has_self_collision = presimulate(robot2)
    # if has_self_collision:
    #     print("Warning: robot self-collides in initial configuration")
    # main_sim = rd.BulletSimulation(task.time_step)
    # task.add_terrain(main_sim)
    # # Rotate 180 degrees around the y axis, so the base points to the right
    # main_sim.add_robot(robot2, robot_init_pos, rd.Quaterniond(0.0, 0.0, 1.0, 0.0))
    # robot_idx = main_sim.find_robot_index(robot2)

    # camera_params, record_step_indices = view_trajectory(
    #     main_sim, robot_idx, input_sequence2, task)